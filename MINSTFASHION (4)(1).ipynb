{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "40960/29515 [=========================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 7s 0us/step\n",
      "26435584/26421880 [==============================] - 7s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n",
      "4431872/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "MaxPooling2D((2, 2)),\n",
    "Conv2D(64, (3, 3), activation='relu'),MaxPooling2D((2, 2)),\n",
    "Conv2D(64, (3, 3), activation='relu'),\n",
    "Flatten(),\n",
    "Dense(64, activation='relu'),\n",
    "Dense(10, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "844/844 [==============================] - 32s 31ms/step - loss: 0.5799 - accuracy: 0.7885 - val_loss: 0.3939 - val_accuracy: 0.8605\n",
      "Epoch 2/10\n",
      "844/844 [==============================] - 27s 32ms/step - loss: 0.3656 - accuracy: 0.8674 - val_loss: 0.3334 - val_accuracy: 0.8787\n",
      "Epoch 3/10\n",
      "844/844 [==============================] - 28s 33ms/step - loss: 0.3106 - accuracy: 0.8865 - val_loss: 0.2997 - val_accuracy: 0.8877\n",
      "Epoch 4/10\n",
      "844/844 [==============================] - 25s 30ms/step - loss: 0.2776 - accuracy: 0.8980 - val_loss: 0.2864 - val_accuracy: 0.8938\n",
      "Epoch 5/10\n",
      "844/844 [==============================] - 25s 30ms/step - loss: 0.2533 - accuracy: 0.9077 - val_loss: 0.2835 - val_accuracy: 0.8947\n",
      "Epoch 6/10\n",
      "844/844 [==============================] - 24s 28ms/step - loss: 0.2316 - accuracy: 0.9137 - val_loss: 0.2668 - val_accuracy: 0.8980\n",
      "Epoch 7/10\n",
      "844/844 [==============================] - 24s 29ms/step - loss: 0.2115 - accuracy: 0.9224 - val_loss: 0.2638 - val_accuracy: 0.9008\n",
      "Epoch 8/10\n",
      "844/844 [==============================] - 26s 31ms/step - loss: 0.1981 - accuracy: 0.9273 - val_loss: 0.2683 - val_accuracy: 0.9032\n",
      "Epoch 9/10\n",
      "844/844 [==============================] - 26s 31ms/step - loss: 0.1838 - accuracy: 0.9314 - val_loss: 0.2733 - val_accuracy: 0.9020\n",
      "Epoch 10/10\n",
      "844/844 [==============================] - 27s 32ms/step - loss: 0.1679 - accuracy: 0.9375 - val_loss: 0.2887 - val_accuracy: 0.9065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2507970f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.2946 - accuracy: 0.9050\n",
      "Test Accuracy: 90.50%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fashion_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('fashion_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted clothing category is: Ankle boot\n"
     ]
    }
   ],
   "source": [
    "#Make predictions on new data\n",
    "# Example input for prediction\n",
    "sample_image = X_test[0:1]\n",
    "# Predict the class probabilities\n",
    "predicted_probabilities = loaded_model.predict(sample_image)\n",
    "# Get the index of the class with the highest probability\n",
    "predicted_class_index = np.argmax(predicted_probabilities)\n",
    "# Define the fashion category labels\n",
    "fashion_labels = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "# Convert the predicted class index to the corresponding fashion category label\n",
    "predicted_label = fashion_labels[predicted_class_index]\n",
    "print(f\"The predicted clothing category is: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the sample image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(sample_image.reshape(28, 28), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Sample Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
